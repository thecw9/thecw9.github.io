---
title: "反向传播算法"
summary: "当我们使用前馈神经网络接收输入 x 并产生输出 yˆ 时，信息通过网络向前流 动。输入 x 提供初始信息，然后传播到每一层的隐藏单元，最终产生输出 yˆ。这称 之为 前向传播（forward propagation）。在训练过程中，前向传播可以持续向前直 到它产生一个标量代价函数 J(θ)。 反向传播（back propagation）算法 (Rumelhart et al., 1986c)，经常简称为backprop，允许来自代价函数的信息通过网络向后流动， 以便计算梯度。 "
categories: ["Product"]
tags: ["roles","career","organization"]
# externalUrl: ""
showSummary: true
date: 2023-03-13
draft: false
showauthor: false
authors:
  - thecw
# series: ["The Complete PM"]
# series\_order: 1
---

在机器学习和深度学习中，使用优化算法找出最优模型参数是实现算法的最具挑战性的任务之一。梯度下降是一种常见的基于梯度的优化算法，但是要实现梯度下降算法，需要计算目标函数的梯度。由于目标函数通常是非线性的，其梯度计算十分复杂。因此，如何高效准确地计算目标函数的梯度一直是机器学习和深度学习中的重要研究方向。计算梯度的解析表达式很多时候往往不可行，但数值化求解这样的表达式的计算代价可能很大。为了解决这个问题，反向传播算法使用简单而廉价的程序来实现这个目标。

反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上， 反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度 来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但是原则上它 可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。


